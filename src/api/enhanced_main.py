"""
åŸºäºxDAN é«˜æ€§èƒ½å¤šæ¨¡æ€æ¨¡å‹xDAN-Vision-SmartDocçš„é«˜æ€§èƒ½å¼‚æ­¥æ™ºèƒ½æ–‡æ¡£è¯†åˆ«ç³»ç»Ÿ
"""

import asyncio
import io
import json
import time
import uuid
from contextlib import asynccontextmanager
from enum import Enum
from typing import Dict, List, Optional, Union

import aioredis
import cv2
import torch
from fastapi import FastAPI, File, Form, HTTPException, UploadFile, Depends, BackgroundTasks, Response
from fastapi.middleware.cors import CORSMiddleware
from fastapi.middleware.gzip import GZipMiddleware
from fastapi.responses import JSONResponse, StreamingResponse, FileResponse
from fastapi.security import HTTPBearer
from fastapi.staticfiles import StaticFiles
from PIL import Image
from pydantic import BaseModel, Field, validator
from transformers import AutoProcessor, VisionEncoderDecoderModel

from ..core.config import settings
from ..core.logging import api_logger, setup_logging
from ..utils.image_utils import ImageDimensions, prepare_image, process_coordinates
from ..utils.parsing_utils import generate_json_output, generate_markdown, parse_layout_string


# ========================= æ•°æ®æ¨¡å‹ =========================

class OutputFormat(str, Enum):
    """è¾“å‡ºæ ¼å¼æšä¸¾"""
    JSON = "json"
    MARKDOWN = "markdown"
    HTML = "html"
    STRUCTURED = "structured"  # åŒ…å«æ‰€æœ‰æ ¼å¼çš„ç»“æ„åŒ–è¾“å‡º


class ProcessingMode(str, Enum):
    """å¤„ç†æ¨¡å¼æšä¸¾"""
    SYNC = "sync"      # åŒæ­¥å¤„ç†ï¼Œç«‹å³è¿”å›ç»“æœ
    ASYNC = "async"    # å¼‚æ­¥å¤„ç†ï¼Œè¿”å›ä»»åŠ¡ID
    STREAM = "stream"  # æµå¼å¤„ç†ï¼Œå®æ—¶è¿”å›è¿›åº¦


class ElementType(str, Enum):
    """å…ƒç´ ç±»å‹æšä¸¾"""
    TEXT = "text"
    TABLE = "table"
    FIGURE = "figure"
    FORMULA = "formula"
    CHART = "chart"
    DIAGRAM = "diagram"


class ProcessingRequest(BaseModel):
    """å¤„ç†è¯·æ±‚æ¨¡å‹"""
    max_batch_size: int = Field(default=16, ge=1, le=64, description="æ‰¹å¤„ç†å¤§å°")
    output_format: OutputFormat = Field(default=OutputFormat.STRUCTURED, description="è¾“å‡ºæ ¼å¼")
    processing_mode: ProcessingMode = Field(default=ProcessingMode.SYNC, description="å¤„ç†æ¨¡å¼")
    include_confidence: bool = Field(default=True, description="æ˜¯å¦åŒ…å«ç½®ä¿¡åº¦")
    include_coordinates: bool = Field(default=True, description="æ˜¯å¦åŒ…å«åæ ‡ä¿¡æ¯")
    include_images: bool = Field(default=False, description="æ˜¯å¦åŒ…å«æå–çš„å›¾åƒ")
    merge_text_blocks: bool = Field(default=True, description="æ˜¯å¦åˆå¹¶ç›¸é‚»æ–‡æœ¬å—")
    
    @validator('max_batch_size')
    def validate_batch_size(cls, v):
        if v < 1 or v > 64:
            raise ValueError('Batch size must be between 1 and 64')
        return v


class ElementResponse(BaseModel):
    """å…ƒç´ å“åº”æ¨¡å‹"""
    element_id: str = Field(description="å…ƒç´ ID")
    type: ElementType = Field(description="å…ƒç´ ç±»å‹")
    bbox: List[float] = Field(description="è¾¹ç•Œæ¡†åæ ‡ [x1, y1, x2, y2]")
    text: str = Field(default="", description="æå–çš„æ–‡æœ¬å†…å®¹")
    confidence: Optional[float] = Field(default=None, ge=0.0, le=1.0, description="ç½®ä¿¡åº¦")
    reading_order: int = Field(description="é˜…è¯»é¡ºåº")
    metadata: Dict = Field(default_factory=dict, description="å…ƒæ•°æ®")
    image_data: Optional[str] = Field(default=None, description="Base64ç¼–ç çš„å›¾åƒæ•°æ®")


class ProcessingResult(BaseModel):
    """å¤„ç†ç»“æœæ¨¡å‹"""
    task_id: str = Field(description="ä»»åŠ¡ID")
    filename: str = Field(description="æ–‡ä»¶å")
    status: str = Field(description="å¤„ç†çŠ¶æ€")
    processing_time: float = Field(description="å¤„ç†æ—¶é—´(ç§’)")
    total_elements: int = Field(description="å…ƒç´ æ€»æ•°")
    elements: List[ElementResponse] = Field(description="è¯†åˆ«çš„å…ƒç´ åˆ—è¡¨")
    markdown_content: Optional[str] = Field(default=None, description="Markdownæ ¼å¼å†…å®¹")
    html_content: Optional[str] = Field(default=None, description="HTMLæ ¼å¼å†…å®¹")
    json_data: Optional[Dict] = Field(default=None, description="ç»“æ„åŒ–JSONæ•°æ®")
    created_at: float = Field(description="åˆ›å»ºæ—¶é—´æˆ³")
    completed_at: Optional[float] = Field(default=None, description="å®Œæˆæ—¶é—´æˆ³")


class TaskStatusResponse(BaseModel):
    """ä»»åŠ¡çŠ¶æ€å“åº”"""
    task_id: str
    status: str
    progress: float = Field(ge=0.0, le=100.0)
    message: Optional[str] = None
    estimated_remaining_time: Optional[float] = None


class HealthResponse(BaseModel):
    """å¥åº·æ£€æŸ¥å“åº”"""
    status: str
    timestamp: float
    version: str
    service_name: str
    components: Dict[str, str]
    performance_metrics: Optional[Dict] = None


# ========================= xDAN Visionå¼•æ“ =========================

class xDANVisionEngine:
    """xDAN Visionæ™ºèƒ½æ–‡æ¡£è¯†åˆ«å¼•æ“"""
    
    def __init__(self):
        self.model = None
        self.processor = None
        self.tokenizer = None
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self._model_loaded = False
        self._load_lock = asyncio.Lock()
        self._processing_semaphore = asyncio.Semaphore(settings.max_concurrent_tasks)
        
        # æ€§èƒ½ç›‘æ§
        self._total_requests = 0
        self._total_processing_time = 0.0
        self._last_request_time = 0.0
        
    async def load_model(self):
        """å¼‚æ­¥æ¨¡å‹åŠ è½½"""
        if self._model_loaded:
            return
            
        async with self._load_lock:
            if self._model_loaded:
                return
                
            api_logger.info("Loading xDAN Vision model", path=settings.model_path)
            start_time = time.time()
            
            # åœ¨çº¿ç¨‹æ± ä¸­åŠ è½½æ¨¡å‹
            loop = asyncio.get_event_loop()
            await loop.run_in_executor(None, self._load_model_sync)
            
            load_time = time.time() - start_time
            self._model_loaded = True
            
            api_logger.info("xDAN Vision model loaded successfully", 
                          load_time=f"{load_time:.2f}s",
                          device=self.device)
    
    def _load_model_sync(self):
        """åŒæ­¥æ¨¡å‹åŠ è½½"""
        try:
            # åŠ è½½Dolphinæ¨¡å‹ç»„ä»¶
            self.processor = AutoProcessor.from_pretrained(settings.model_path)
            self.model = VisionEncoderDecoderModel.from_pretrained(settings.model_path)
            
            # æ¨¡å‹ä¼˜åŒ–
            self.model.eval()
            self.model.to(self.device)
            
            # ç²¾åº¦ä¼˜åŒ–
            if settings.model_precision == "half" and self.device != "cpu":
                self.model = self.model.half()
            
            # ç¼–è¯‘ä¼˜åŒ– (PyTorch 2.0+)
            if hasattr(torch, 'compile') and settings.model_device == "cuda":
                try:
                    self.model = torch.compile(self.model, mode="reduce-overhead")
                    api_logger.info("Model compiled with torch.compile")
                except Exception as e:
                    api_logger.warning("Failed to compile model", error=str(e))
            
            self.tokenizer = self.processor.tokenizer
            
            # GPUå†…å­˜ä¼˜åŒ–
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
                
        except Exception as e:
            api_logger.error("Failed to load xDAN Vision model", error=str(e))
            raise
    
    async def process_document(
        self, 
        image: Image.Image, 
        request: ProcessingRequest,
        background_tasks: BackgroundTasks = None
    ) -> ProcessingResult:
        """æ–‡æ¡£å¤„ç†ä¸»å…¥å£"""
        
        async with self._processing_semaphore:
            start_time = time.time()
            task_id = str(uuid.uuid4())
            
            try:
                # æ›´æ–°æ€§èƒ½æŒ‡æ ‡
                self._total_requests += 1
                self._last_request_time = start_time
                
                # æ‰§è¡Œå¤„ç†
                if request.processing_mode == ProcessingMode.SYNC:
                    result = await self._process_sync(image, request, task_id, start_time)
                elif request.processing_mode == ProcessingMode.ASYNC:
                    result = await self._process_async(image, request, task_id, start_time, background_tasks)
                else:  # STREAM
                    result = await self._process_stream(image, request, task_id, start_time)
                
                # æ›´æ–°æ€§èƒ½ç»Ÿè®¡
                processing_time = time.time() - start_time
                self._total_processing_time += processing_time
                
                return result
                
            except Exception as e:
                api_logger.error("xDAN Vision document processing failed", 
                               task_id=task_id, error=str(e))
                raise
    
    async def _process_sync(
        self, 
        image: Image.Image, 
        request: ProcessingRequest,
        task_id: str,
        start_time: float
    ) -> ProcessingResult:
        """åŒæ­¥å¤„ç†æ¨¡å¼"""
        
        # é˜¶æ®µ1: å¸ƒå±€è§£æ
        layout_output = await self.chat("Parse the reading order of this document.", image)
        
        # é˜¶æ®µ2: å…ƒç´ å¤„ç†
        loop = asyncio.get_event_loop()
        padded_image, dims = await loop.run_in_executor(None, prepare_image, image)
        
        elements = await self._process_elements(
            layout_output, padded_image, dims, request
        )
        
        # æ ¼å¼åŒ–è¾“å‡º
        return await self._format_result(
            elements, task_id, request, start_time, "unknown.png"
        )
    
    async def _process_elements(
        self, 
        layout_results: str, 
        padded_image, 
        dims: ImageDimensions, 
        request: ProcessingRequest
    ) -> List[Dict]:
        """å¤„ç†æ–‡æ¡£å…ƒç´ """
        
        loop = asyncio.get_event_loop()
        layout_data = await loop.run_in_executor(None, parse_layout_string, layout_results)
        
        # åˆ†ç±»æ”¶é›†å…ƒç´ 
        element_groups = {
            "text": [],
            "table": [], 
            "figure": [],
            "formula": []
        }
        
        reading_order = 0
        
        for bbox, label in layout_data:
            try:
                # å¤„ç†åæ ‡
                coords_result = await loop.run_in_executor(
                    None, process_coordinates, bbox, padded_image, dims, None
                )
                x1, y1, x2, y2, orig_x1, orig_y1, orig_x2, orig_y2, _ = coords_result
                
                # è£å‰ªå…ƒç´ 
                cropped = padded_image[y1:y2, x1:x2]
                if cropped.size > 0:
                    element_info = {
                        "bbox": [orig_x1, orig_y1, orig_x2, orig_y2],
                        "reading_order": reading_order,
                        "element_id": str(uuid.uuid4())
                    }
                    
                    if label == "fig":
                        element_info.update({
                            "type": "figure",
                            "text": "",
                            "confidence": 1.0
                        })
                        
                        # å¦‚æœéœ€è¦åŒ…å«å›¾åƒæ•°æ®
                        if request.include_images:
                            pil_crop = Image.fromarray(cv2.cvtColor(cropped, cv2.COLOR_BGR2RGB))
                            element_info["image_data"] = await self._encode_image_base64(pil_crop)
                        
                        element_groups["figure"].append(element_info)
                    else:
                        pil_crop = Image.fromarray(cv2.cvtColor(cropped, cv2.COLOR_BGR2RGB))
                        element_info["crop"] = pil_crop
                        
                        if label == "tab":
                            element_info["type"] = "table"
                            element_groups["table"].append(element_info)
                        else:
                            element_info["type"] = "text"
                            element_groups["text"].append(element_info)
                
                reading_order += 1
                
            except Exception as e:
                api_logger.warning("Failed to process element", 
                                 label=label, error=str(e))
                continue
        
        # æ‰¹é‡å¤„ç†å„ç±»å…ƒç´ 
        all_elements = element_groups["figure"].copy()
        
        # å¤„ç†æ–‡æœ¬å…ƒç´ 
        if element_groups["text"]:
            text_results = await self._process_element_batch(
                element_groups["text"], "Read text in the image.", request
            )
            all_elements.extend(text_results)
        
        # å¤„ç†è¡¨æ ¼å…ƒç´ 
        if element_groups["table"]:
            table_results = await self._process_element_batch(
                element_groups["table"], "Parse the table in the image.", request
            )
            all_elements.extend(table_results)
        
        # æ’åºå¹¶åå¤„ç†
        all_elements.sort(key=lambda x: x["reading_order"])
        
        # åˆå¹¶ç›¸é‚»æ–‡æœ¬å—
        if request.merge_text_blocks:
            all_elements = await self._merge_adjacent_text_blocks(all_elements)
        
        return all_elements
    
    async def _process_element_batch(
        self, 
        elements: List[Dict], 
        prompt: str, 
        request: ProcessingRequest
    ) -> List[Dict]:
        """æ‰¹é‡å¤„ç†å…ƒç´ """
        results = []
        
        for i in range(0, len(elements), request.max_batch_size):
            batch = elements[i:i + request.max_batch_size]
            crops = [elem["crop"] for elem in batch]
            prompts = [prompt] * len(crops)
            
            try:
                # æ‰¹é‡æ¨ç†
                texts = await self.chat_batch(prompts, crops)
                
                # æ„å»ºç»“æœ
                for elem, text in zip(batch, texts):
                    result = {
                        "element_id": elem["element_id"],
                        "type": elem["type"],
                        "bbox": elem["bbox"],
                        "text": text.strip(),
                        "reading_order": elem["reading_order"],
                        "metadata": {}
                    }
                    
                    # æ·»åŠ ç½®ä¿¡åº¦ï¼ˆç®€åŒ–ç‰ˆï¼Œå®é™…åº”è¯¥ä»æ¨¡å‹è·å–ï¼‰
                    if request.include_confidence:
                        result["confidence"] = min(1.0, len(text) / 100.0)  # ç®€åŒ–è®¡ç®—
                    
                    # æ·»åŠ å›¾åƒæ•°æ®
                    if request.include_images and "crop" in elem:
                        result["image_data"] = await self._encode_image_base64(elem["crop"])
                    
                    results.append(result)
                    
            except Exception as e:
                api_logger.error("Batch processing failed", error=str(e))
                # æ·»åŠ ç©ºç»“æœ
                for elem in batch:
                    results.append({
                        "element_id": elem["element_id"],
                        "type": elem["type"],
                        "bbox": elem["bbox"],
                        "text": "",
                        "reading_order": elem["reading_order"],
                        "confidence": 0.0,
                        "metadata": {"error": "Processing failed"}
                    })
        
        return results
    
    async def _format_result(
        self,
        elements: List[Dict],
        task_id: str,
        request: ProcessingRequest,
        start_time: float,
        filename: str
    ) -> ProcessingResult:
        """æ ¼å¼åŒ–ç»“æœ - æ”¯æŒå¤šç§è¾“å‡ºæ ¼å¼"""
        
        processing_time = time.time() - start_time
        completed_at = time.time()
        
        # è½¬æ¢ä¸ºå“åº”æ¨¡å‹
        element_responses = []
        for elem in elements:
            element_responses.append(ElementResponse(
                element_id=elem["element_id"],
                type=ElementType(elem["type"]),
                bbox=elem["bbox"],
                text=elem["text"],
                confidence=elem.get("confidence") if request.include_confidence else None,
                reading_order=elem["reading_order"],
                metadata=elem.get("metadata", {}),
                image_data=elem.get("image_data") if request.include_images else None
            ))
        
        # åŸºç¡€ç»“æœ
        result = ProcessingResult(
            task_id=task_id,
            filename=filename,
            status="completed",
            processing_time=processing_time,
            total_elements=len(elements),
            elements=element_responses,
            created_at=start_time,
            completed_at=completed_at
        )
        
        # æ ¹æ®è¾“å‡ºæ ¼å¼æ·»åŠ å†…å®¹
        if request.output_format in [OutputFormat.MARKDOWN, OutputFormat.STRUCTURED]:
            result.markdown_content = generate_markdown(elements)
        
        if request.output_format in [OutputFormat.HTML, OutputFormat.STRUCTURED]:
            result.html_content = await self._generate_html(elements)
        
        if request.output_format in [OutputFormat.JSON, OutputFormat.STRUCTURED]:
            result.json_data = generate_json_output(elements, processing_time)
        
        return result
    
    async def _generate_html(self, elements: List[Dict]) -> str:
        """ç”ŸæˆHTMLæ ¼å¼è¾“å‡º"""
        html_parts = [
            "<!DOCTYPE html>",
            "<html lang='zh-CN'>",
            "<head>",
            "    <meta charset='UTF-8'>",
            "    <meta name='viewport' content='width=device-width, initial-scale=1.0'>",
            "    <title>xDAN Vision Document Analysis Result</title>",
            "    <style>",
            "        .document-container { max-width: 800px; margin: 0 auto; padding: 20px; }",
            "        .header { text-align: center; margin-bottom: 30px; }",
            "        .element { margin: 15px 0; padding: 10px; border-left: 3px solid #007acc; }",
            "        .text-element { font-family: Arial, sans-serif; background-color: #f8f9fa; }",
            "        .table-element { background-color: #fff3cd; }",
            "        .figure-element { text-align: center; background-color: #d1ecf1; }",
            "        .formula-element { text-align: center; background-color: #d4edda; }",
            "        .confidence-indicator { ",
            "            display: inline-block; width: 10px; height: 10px; ",
            "            border-radius: 50%; margin-right: 8px; ",
            "        }",
            "        .high-confidence { background-color: #28a745; }",
            "        .medium-confidence { background-color: #ffc107; }",
            "        .low-confidence { background-color: #dc3545; }",
            "        table { border-collapse: collapse; width: 100%; }",
            "        th, td { border: 1px solid #ddd; padding: 8px; text-align: left; }",
            "        th { background-color: #f2f2f2; }",
            "    </style>",
            "</head>",
            "<body>",
            "    <div class='document-container'>",
            "        <div class='header'>",
            "            <h1>ğŸ“„ xDAN Vision æ–‡æ¡£åˆ†æç»“æœ</h1>",
            "            <p>æ™ºèƒ½æ–‡æ¡£è¯†åˆ«ä¸ç»“æ„åŒ–è§£æ</p>",
            "        </div>"
        ]
        
        for element in sorted(elements, key=lambda x: x["reading_order"]):
            elem_type = element["type"]
            text = element["text"]
            bbox = element["bbox"]
            confidence = element.get("confidence", 0.0)
            
            # ç½®ä¿¡åº¦æŒ‡ç¤ºå™¨
            if confidence >= 0.8:
                confidence_class = "high-confidence"
            elif confidence >= 0.5:
                confidence_class = "medium-confidence" 
            else:
                confidence_class = "low-confidence"
            
            # æ·»åŠ å…ƒç´ å®¹å™¨
            html_parts.append(f'        <div class="element {elem_type}-element" data-bbox="{bbox}">')
            html_parts.append(f'            <span class="confidence-indicator {confidence_class}" title="Confidence: {confidence:.2f}"></span>')
            
            if elem_type == "text":
                html_parts.append(f"            <p>{text}</p>")
            elif elem_type == "table":
                # å°è¯•è§£æè¡¨æ ¼ç»“æ„
                if "|" in text:
                    html_parts.append("            " + self._markdown_table_to_html(text))
                else:
                    html_parts.append(f"            <pre class='table-content'>{text}</pre>")
            elif elem_type == "figure":
                html_parts.append("            <div class='figure-placeholder'>")
                html_parts.append("                ğŸ“Š <strong>å›¾è¡¨/å›¾åƒ</strong>")
                if text:
                    html_parts.append(f"                <p>{text}</p>")
                html_parts.append("            </div>")
            elif elem_type == "formula":
                html_parts.append(f"            <div class='formula'>ğŸ“ <code>{text}</code></div>")
            
            html_parts.append("        </div>")
        
        html_parts.extend([
            "    </div>",
            "</body>",
            "</html>"
        ])
        
        return "\n".join(html_parts)
    
    def _markdown_table_to_html(self, markdown_table: str) -> str:
        """å°†Markdownè¡¨æ ¼è½¬æ¢ä¸ºHTML"""
        lines = [line.strip() for line in markdown_table.split('\n') if line.strip()]
        if not lines:
            return "<table></table>"
        
        html = ["<table>"]
        
        for i, line in enumerate(lines):
            if line.startswith('|') and line.endswith('|'):
                cells = [cell.strip() for cell in line[1:-1].split('|')]
                
                if i == 1 and all(set(cell.strip()) <= {'-', ':', ' '} for cell in cells):
                    continue  # Skip separator line
                
                tag = "th" if i == 0 else "td"
                html.append("    <tr>")
                for cell in cells:
                    html.append(f"        <{tag}>{cell}</{tag}>")
                html.append("    </tr>")
        
        html.append("</table>")
        return "\n".join(html)
    
    async def _encode_image_base64(self, image: Image.Image) -> str:
        """å°†å›¾åƒç¼–ç ä¸ºBase64å­—ç¬¦ä¸²"""
        import base64
        buffer = io.BytesIO()
        image.save(buffer, format='PNG')
        img_str = base64.b64encode(buffer.getvalue()).decode()
        return f"data:image/png;base64,{img_str}"
    
    async def _merge_adjacent_text_blocks(self, elements: List[Dict]) -> List[Dict]:
        """åˆå¹¶ç›¸é‚»çš„æ–‡æœ¬å—"""
        if not elements:
            return elements
        
        merged = []
        current_group = []
        
        for element in sorted(elements, key=lambda x: x["reading_order"]):
            if element["type"] == "text":
                if not current_group:
                    current_group.append(element)
                else:
                    # æ£€æŸ¥æ˜¯å¦ç›¸é‚»ï¼ˆç®€åŒ–é€»è¾‘ï¼‰
                    last_elem = current_group[-1]
                    if self._are_adjacent(last_elem["bbox"], element["bbox"]):
                        current_group.append(element)
                    else:
                        # åˆå¹¶å½“å‰ç»„å¹¶å¼€å§‹æ–°ç»„
                        if len(current_group) > 1:
                            merged.append(self._merge_text_group(current_group))
                        else:
                            merged.extend(current_group)
                        current_group = [element]
            else:
                # å¤„ç†éæ–‡æœ¬å…ƒç´ 
                if current_group:
                    if len(current_group) > 1:
                        merged.append(self._merge_text_group(current_group))
                    else:
                        merged.extend(current_group)
                    current_group = []
                merged.append(element)
        
        # å¤„ç†æœ€åä¸€ç»„
        if current_group:
            if len(current_group) > 1:
                merged.append(self._merge_text_group(current_group))
            else:
                merged.extend(current_group)
        
        return merged
    
    def _are_adjacent(self, bbox1: List[float], bbox2: List[float], threshold: float = 20.0) -> bool:
        """æ£€æŸ¥ä¸¤ä¸ªè¾¹ç•Œæ¡†æ˜¯å¦ç›¸é‚»"""
        # ç®€åŒ–çš„ç›¸é‚»æ£€æµ‹é€»è¾‘
        vertical_distance = abs(bbox2[1] - bbox1[3])  # ä¸Šä¸‹è·ç¦»
        return vertical_distance <= threshold
    
    def _merge_text_group(self, group: List[Dict]) -> Dict:
        """åˆå¹¶æ–‡æœ¬ç»„"""
        if not group:
            return {}
        
        # åˆå¹¶æ–‡æœ¬
        texts = [elem["text"] for elem in group if elem["text"].strip()]
        merged_text = " ".join(texts)
        
        # è®¡ç®—åˆå¹¶è¾¹ç•Œæ¡†
        min_x = min(elem["bbox"][0] for elem in group)
        min_y = min(elem["bbox"][1] for elem in group)
        max_x = max(elem["bbox"][2] for elem in group)
        max_y = max(elem["bbox"][3] for elem in group)
        
        # ä½¿ç”¨ç¬¬ä¸€ä¸ªå…ƒç´ ä½œä¸ºåŸºç¡€
        merged = group[0].copy()
        merged.update({
            "text": merged_text,
            "bbox": [min_x, min_y, max_x, max_y],
            "metadata": {"merged_from": len(group)}
        })
        
        return merged
    
    async def chat(self, prompt: str, image: Image.Image) -> str:
        """å•å›¾åƒå¤„ç†"""
        if not self._model_loaded:
            await self.load_model()
            
        loop = asyncio.get_event_loop()
        return await loop.run_in_executor(None, self._chat_sync, prompt, image)
    
    async def chat_batch(self, prompts: List[str], images: List[Image.Image]) -> List[str]:
        """æ‰¹é‡å›¾åƒå¤„ç†"""
        if not self._model_loaded:
            await self.load_model()
            
        loop = asyncio.get_event_loop()
        return await loop.run_in_executor(None, self._chat_batch_sync, prompts, images)
    
    def _chat_sync(self, prompt: str, image: Image.Image) -> str:
        """åŒæ­¥å•å›¾åƒå¤„ç†"""
        try:
            # å‡†å¤‡è¾“å…¥
            pixel_values = self.processor(image, return_tensors="pt").pixel_values
            if settings.model_precision == "half" and self.device != "cpu":
                pixel_values = pixel_values.half()
            pixel_values = pixel_values.to(self.device)
            
            # å‡†å¤‡æç¤º
            formatted_prompt = f"<s>{prompt} <Answer/>"
            prompt_ids = self.tokenizer(
                formatted_prompt, 
                add_special_tokens=False, 
                return_tensors="pt"
            ).input_ids.to(self.device)
            
            # ç”Ÿæˆ
            with torch.inference_mode():
                outputs = self.model.generate(
                    pixel_values=pixel_values,
                    decoder_input_ids=prompt_ids,
                    min_length=1,
                    max_length=settings.max_sequence_length,
                    pad_token_id=self.tokenizer.pad_token_id,
                    eos_token_id=self.tokenizer.eos_token_id,
                    use_cache=True,
                    bad_words_ids=[[self.tokenizer.unk_token_id]],
                    do_sample=False,
                    num_beams=1,
                    repetition_penalty=1.1
                )
            
            # è§£ç 
            sequence = self.tokenizer.batch_decode(outputs.sequences, skip_special_tokens=False)[0]
            result = sequence.replace(formatted_prompt, "").replace("<pad>", "").replace("</s>", "").strip()
            
            return result
            
        except Exception as e:
            api_logger.error("Single chat processing failed", error=str(e))
            return ""
        finally:
            # æ¸…ç†GPUå†…å­˜
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
    
    def _chat_batch_sync(self, prompts: List[str], images: List[Image.Image]) -> List[str]:
        """åŒæ­¥æ‰¹é‡å¤„ç†"""
        try:
            # å‡†å¤‡æ‰¹é‡è¾“å…¥
            batch_inputs = self.processor(images, return_tensors="pt", padding=True)
            batch_pixel_values = batch_inputs.pixel_values
            
            if settings.model_precision == "half" and self.device != "cpu":
                batch_pixel_values = batch_pixel_values.half()
            batch_pixel_values = batch_pixel_values.to(self.device)
            
            # å‡†å¤‡æ‰¹é‡æç¤º
            formatted_prompts = [f"<s>{p} <Answer/>" for p in prompts]
            batch_prompt_inputs = self.tokenizer(
                formatted_prompts,
                add_special_tokens=False,
                return_tensors="pt",
                padding=True
            )
            
            batch_prompt_ids = batch_prompt_inputs.input_ids.to(self.device)
            batch_attention_mask = batch_prompt_inputs.attention_mask.to(self.device)
            
            # æ‰¹é‡ç”Ÿæˆ
            with torch.inference_mode():
                outputs = self.model.generate(
                    pixel_values=batch_pixel_values,
                    decoder_input_ids=batch_prompt_ids,
                    decoder_attention_mask=batch_attention_mask,
                    min_length=1,
                    max_length=settings.max_sequence_length,
                    pad_token_id=self.tokenizer.pad_token_id,
                    eos_token_id=self.tokenizer.eos_token_id,
                    use_cache=True,
                    bad_words_ids=[[self.tokenizer.unk_token_id]],
                    do_sample=False,
                    num_beams=1,
                    repetition_penalty=1.1
                )
            
            # æ‰¹é‡è§£ç 
            sequences = self.tokenizer.batch_decode(outputs.sequences, skip_special_tokens=False)
            results = []
            for i, sequence in enumerate(sequences):
                cleaned = sequence.replace(formatted_prompts[i], "").replace("<pad>", "").replace("</s>", "").strip()
                results.append(cleaned)
                
            return results
            
        except Exception as e:
            api_logger.error("Batch chat processing failed", error=str(e))
            return [""] * len(prompts)
        finally:
            # æ¸…ç†GPUå†…å­˜
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
    
    def get_performance_metrics(self) -> Dict:
        """è·å–æ€§èƒ½æŒ‡æ ‡"""
        if self._total_requests == 0:
            return {"total_requests": 0}
        
        avg_processing_time = self._total_processing_time / self._total_requests
        
        return {
            "total_requests": self._total_requests,
            "average_processing_time": round(avg_processing_time, 3),
            "total_processing_time": round(self._total_processing_time, 3),
            "last_request_time": self._last_request_time,
            "model_loaded": self._model_loaded,
            "device": self.device
        }


# ========================= å…¨å±€å˜é‡å’Œä¾èµ– =========================

vision_engine = None
redis_client = None
security = HTTPBearer(auto_error=False)


async def get_engine() -> xDANVisionEngine:
    """è·å–å¼•æ“å®ä¾‹"""
    global vision_engine
    if not vision_engine:
        raise HTTPException(status_code=503, detail="xDAN Vision service not ready")
    return vision_engine


async def get_redis() -> Optional[aioredis.Redis]:
    """è·å–Rediså®¢æˆ·ç«¯"""
    return redis_client


# ========================= åº”ç”¨ç”Ÿå‘½å‘¨æœŸ =========================

@asynccontextmanager
async def lifespan(app: FastAPI):
    """åº”ç”¨ç”Ÿå‘½å‘¨æœŸç®¡ç†"""
    global vision_engine, redis_client
    
    # å¯åŠ¨æ—¶åˆå§‹åŒ–
    setup_logging()
    api_logger.info("Starting xDAN-Vision-SmartDoc API Server")
    
    # åˆå§‹åŒ–xDAN Visionå¼•æ“
    vision_engine = xDANVisionEngine()
    
    # åˆå§‹åŒ–Redis
    try:
        redis_client = aioredis.from_url(settings.redis_url)
        await redis_client.ping()
        api_logger.info("Redis connected successfully")
    except Exception as e:
        api_logger.warning("Redis connection failed", error=str(e))
        redis_client = None
    
    yield
    
    # å…³é—­æ—¶æ¸…ç†
    if redis_client:
        await redis_client.close()
    api_logger.info("xDAN-Vision-SmartDoc API Server stopped")


# ========================= FastAPIåº”ç”¨ =========================

app = FastAPI(
    title="xDAN-Vision-SmartDoc API Server",
    version="2.0.0",
    description="ğŸ”¬ åŸºäºxDAN é«˜æ€§èƒ½å¤šæ¨¡æ€æ¨¡å‹xDAN-Vision-SmartDocçš„é«˜æ€§èƒ½å¼‚æ­¥æ™ºèƒ½æ–‡æ¡£è¯†åˆ«ç³»ç»Ÿ\n\n"
    "## ğŸš€ æ ¸å¿ƒåŠŸèƒ½\n"
    "- ğŸ“ **OCRæ–‡æœ¬è¯†åˆ«**: é«˜ç²¾åº¦æ–‡æœ¬æå–å’Œè¯†åˆ«\n"
    "- ğŸ“Š **è¡¨æ ¼è§£æ**: æ™ºèƒ½è¡¨æ ¼ç»“æ„è¯†åˆ«å’Œå†…å®¹æå–\n"
    "- ğŸ–¼ï¸ **å›¾è¡¨åˆ†æ**: å›¾è¡¨ã€å›¾åƒå†…å®¹ç†è§£å’Œæè¿°\n"
    "- ğŸ§® **å…¬å¼è¯†åˆ«**: æ•°å­¦å…¬å¼å’Œç§‘å­¦ç¬¦å·è¯†åˆ«\n"
    "- ğŸ“‹ **ç‰ˆé¢åˆ†æ**: æ–‡æ¡£ç»“æ„å’Œé˜…è¯»é¡ºåºåˆ†æ\n"
    "- ğŸ¯ **å¤šæ ¼å¼è¾“å‡º**: JSON/Markdown/HTML/ç»“æ„åŒ–æ•°æ®\n\n"
    "## âš¡ æ€§èƒ½ç‰¹è‰²\n"
    "- ğŸ”¥ **å¼‚æ­¥å¤„ç†**: é«˜å¹¶å‘è¯·æ±‚å¤„ç†èƒ½åŠ›\n"
    "- ğŸš€ **æ‰¹é‡è¯†åˆ«**: æ™ºèƒ½æ‰¹å¤„ç†ä¼˜åŒ–\n"
    "- ğŸ’¾ **æ™ºèƒ½ç¼“å­˜**: Redisç¼“å­˜åŠ é€Ÿ\n"
    "- ğŸ“Š **å®æ—¶ç›‘æ§**: æ€§èƒ½æŒ‡æ ‡å’Œå¥åº·æ£€æŸ¥\n"
    "- ğŸ”§ **ä¼ä¸šçº§**: ç”Ÿäº§ç¯å¢ƒä¼˜åŒ–é…ç½®\n\n"
    "## ğŸ“– ä½¿ç”¨è¯´æ˜\n"
    "1. ğŸ“± **ç½‘é¡µç‰ˆ**: è®¿é—® `/web` è¿›è¡Œå¯è§†åŒ–æ“ä½œ\n"
    "2. ğŸ”§ **APIè°ƒç”¨**: ä½¿ç”¨ `/api/v2/document/analyze` ç«¯ç‚¹\n"
    "3. ğŸ“š **æ–‡æ¡£**: æŸ¥çœ‹å®Œæ•´APIæ–‡æ¡£å’Œç¤ºä¾‹\n"
    "4. ğŸ¯ **æ ¼å¼é€‰æ‹©**: æ ¹æ®éœ€æ±‚é€‰æ‹©æœ€é€‚åˆçš„è¾“å‡ºæ ¼å¼",
    contact={
        "name": "xDAN-Vision-SmartDoc æŠ€æœ¯æ”¯æŒ",
        "url": "https://github.com/xDAN-AI/xDAN-smartDoc-dolphin",
        "email": "support@xdan.ai",
    },
    license_info={
        "name": "MIT License",
        "url": "https://opensource.org/licenses/MIT",
    },
    openapi_tags=[
        {
            "name": "document",
            "description": "æ–‡æ¡£è¯†åˆ«ç›¸å…³æ“ä½œ",
        },
        {
            "name": "system",
            "description": "ç³»ç»Ÿç›‘æ§å’Œå¥åº·æ£€æŸ¥",
        },
        {
            "name": "web",
            "description": "Webç•Œé¢å’Œé™æ€èµ„æº",
        },
    ],
    lifespan=lifespan,
    docs_url="/docs",
    redoc_url="/redoc",
)

# ä¸­é—´ä»¶é…ç½®
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # ç”Ÿäº§ç¯å¢ƒåº”é™åˆ¶å…·ä½“åŸŸå
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

app.add_middleware(GZipMiddleware, minimum_size=1000)

# é™æ€æ–‡ä»¶æœåŠ¡
app.mount("/static", StaticFiles(directory="src/web"), name="static")


# ========================= APIè·¯ç”± =========================

@app.get("/", tags=["system"])
async def root():
    """æ ¹è·¯å¾„ä¿¡æ¯"""
    return {
        "service": "xDAN-Vision-SmartDoc API Server",
        "version": "2.0.0",
        "description": "åŸºäºxDAN é«˜æ€§èƒ½å¤šæ¨¡æ€æ¨¡å‹xDAN-Vision-SmartDocçš„æ™ºèƒ½æ–‡æ¡£è¯†åˆ«ç³»ç»Ÿ",
        "endpoints": {
            "web_interface": "/web",
            "api_docs": "/docs",
            "redoc_docs": "/redoc",
            "health_check": "/health",
            "document_analysis": "/api/v2/document/analyze"
        },
        "features": [
            "OCRæ–‡æœ¬è¯†åˆ«",
            "è¡¨æ ¼è§£æ", 
            "å›¾è¡¨åˆ†æ",
            "å…¬å¼è¯†åˆ«",
            "å¤šæ ¼å¼è¾“å‡º",
            "å¼‚æ­¥å¤„ç†"
        ]
    }


@app.get("/health", response_model=HealthResponse)
async def health_check(engine: xDANVisionEngine = Depends(get_engine)):
    """å¥åº·æ£€æŸ¥"""
    performance_metrics = engine.get_performance_metrics()
    
    health_response = HealthResponse(
        status="healthy",
        timestamp=time.time(),
        version="2.0.0",
        service_name="xDAN-Vision-SmartDoc",
        components={
            "vision_engine": "loaded" if engine._model_loaded else "not_loaded",
            "redis_cache": "connected" if redis_client else "disconnected",
            "compute_device": engine.device,
            "model_precision": settings.model_precision
        },
        performance_metrics=performance_metrics
    )
    
    return health_response


@app.post("/api/process/document", response_model=ProcessingResult)
async def process_document(
    file: UploadFile = File(..., description="ğŸ“„ æ–‡æ¡£å›¾åƒæ–‡ä»¶ (æ”¯æŒPNG, JPG, JPEG)"),
    request: ProcessingRequest = Depends(),
    background_tasks: BackgroundTasks = BackgroundTasks(),
    engine: xDANVisionEngine = Depends(get_engine)
):
    """
    ğŸ”¬ **æ™ºèƒ½æ–‡æ¡£å¤„ç†API**
    
    **åŠŸèƒ½æè¿°:**
    - ğŸ“ OCRæ–‡æœ¬è¯†åˆ«ä¸æå–
    - ğŸ“Š è¡¨æ ¼ç»“æ„è§£æä¸æ ¼å¼åŒ–  
    - ğŸ–¼ï¸ å›¾è¡¨/å›¾åƒåˆ†æä¸æè¿°
    - ğŸ§® æ•°å­¦å…¬å¼è¯†åˆ«(LaTeXæ ¼å¼)
    - ğŸ“‹ æ™ºèƒ½å¸ƒå±€åˆ†æä¸é˜…è¯»é¡ºåº
    
    **å‚æ•°è¯´æ˜:**
    - **file**: ä¸Šä¼ çš„æ–‡æ¡£å›¾åƒæ–‡ä»¶
    - **max_batch_size**: æ‰¹å¤„ç†å¤§å° (1-64)ï¼Œå½±å“å¤„ç†é€Ÿåº¦
    - **output_format**: è¾“å‡ºæ ¼å¼é€‰æ‹©
        - `json`: ç»“æ„åŒ–æ•°æ®ï¼Œé€‚åˆç¨‹åºå¤„ç†
        - `markdown`: å¯è¯»æ€§æ–‡æ¡£ï¼Œé€‚åˆç¼–è¾‘
        - `html`: ç½‘é¡µæ˜¾ç¤ºï¼Œé€‚åˆé¢„è§ˆ
        - `structured`: åŒ…å«æ‰€æœ‰æ ¼å¼çš„å®Œæ•´è¾“å‡º
    - **processing_mode**: å¤„ç†æ¨¡å¼ (sync/async/stream)
    - **include_confidence**: æ˜¯å¦åŒ…å«è¯†åˆ«ç½®ä¿¡åº¦
    - **include_coordinates**: æ˜¯å¦åŒ…å«å…ƒç´ åæ ‡ä¿¡æ¯
    - **include_images**: æ˜¯å¦åŒ…å«æå–çš„å›¾åƒæ•°æ®(Base64)
    - **merge_text_blocks**: æ˜¯å¦åˆå¹¶ç›¸é‚»æ–‡æœ¬å—
    
    **è¿”å›ç»“æœ:**
    åŒ…å«è¯†åˆ«çš„æ–‡æ¡£å…ƒç´ ã€æ ¼å¼åŒ–å†…å®¹å’Œå¤„ç†ç»Ÿè®¡ä¿¡æ¯
    """
    
    # æ–‡ä»¶éªŒè¯
    if not file.content_type or not file.content_type.startswith('image/'):
        raise HTTPException(
            status_code=400, 
            detail="âŒ æ–‡ä»¶æ ¼å¼é”™è¯¯: è¯·ä¸Šä¼ å›¾åƒæ–‡ä»¶ (æ”¯æŒPNG, JPG, JPEG)"
        )
    
    try:
        # è¯»å–å›¾åƒ
        contents = await file.read()
        image = Image.open(io.BytesIO(contents)).convert("RGB")
        
        # å¤„ç†æ–‡æ¡£
        result = await engine.process_document(
            image=image,
            request=request,
            background_tasks=background_tasks
        )
        
        # æ›´æ–°æ–‡ä»¶å
        result.filename = file.filename or "unknown.png"
        
        # ç¼“å­˜ç»“æœ
        if redis_client and request.processing_mode == ProcessingMode.SYNC:
            try:
                await redis_client.setex(
                    f"xdan:result:{result.task_id}",
                    3600,  # 1å°æ—¶è¿‡æœŸ
                    result.json()
                )
            except Exception as e:
                api_logger.warning("Failed to cache result", error=str(e))
        
        api_logger.info("Document processed successfully",
                       task_id=result.task_id,
                       filename=file.filename,
                       total_elements=result.total_elements,
                       processing_time=result.processing_time,
                       output_format=request.output_format.value)
        
        return result
        
    except Exception as e:
        api_logger.error("xDAN Vision document processing failed", error=str(e))
        raise HTTPException(status_code=500, detail=f"âŒ å¤„ç†å¤±è´¥: {str(e)}")


@app.get("/api/result/{task_id}", response_model=ProcessingResult)
async def get_result(
    task_id: str,
    redis_client: Optional[aioredis.Redis] = Depends(get_redis)
):
    """ğŸ“„ è·å–å¤„ç†ç»“æœ"""
    if not redis_client:
        raise HTTPException(status_code=503, detail="âŒ ç»“æœå­˜å‚¨æœåŠ¡ä¸å¯ç”¨")
    
    try:
        result_data = await redis_client.get(f"xdan:result:{task_id}")
        if not result_data:
            raise HTTPException(status_code=404, detail="âŒ æœªæ‰¾åˆ°æŒ‡å®šä»»åŠ¡ç»“æœ")
        
        # è§£æJSONç»“æœ
        result_dict = json.loads(result_data)
        return ProcessingResult(**result_dict)
        
    except json.JSONDecodeError:
        raise HTTPException(status_code=500, detail="âŒ ç»“æœæ•°æ®æ ¼å¼é”™è¯¯")
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"âŒ è·å–ç»“æœå¤±è´¥: {str(e)}")


@app.get("/api/metrics", response_model=Dict)
async def get_performance_metrics(engine: xDANVisionEngine = Depends(get_engine)):
    """ğŸ“Š è·å–ç³»ç»Ÿæ€§èƒ½æŒ‡æ ‡"""
    return engine.get_performance_metrics()


@app.get("/api/formats", response_model=Dict[str, Any])
async def get_supported_formats():
    """ğŸ“‹ è·å–æ”¯æŒçš„æ ¼å¼ä¿¡æ¯"""
    return {
        "input_formats": {
            "images": ["PNG", "JPG", "JPEG"],
            "max_file_size": "50MB",
            "recommended_dpi": "300 DPI"
        },
        "output_formats": {
            "json": {
                "description": "ç»“æ„åŒ–æ•°æ®æ ¼å¼",
                "use_case": "ç¨‹åºå¤„ç†ã€APIå¯¹æ¥",
                "features": ["å…ƒç´ ä¿¡æ¯", "åæ ‡æ•°æ®", "ç½®ä¿¡åº¦"]
            },
            "markdown": {
                "description": "å¯è¯»æ€§æ–‡æ¡£æ ¼å¼", 
                "use_case": "æ–‡æ¡£ç¼–è¾‘ã€ç‰ˆæœ¬æ§åˆ¶",
                "features": ["ä¿æŒæ ¼å¼", "æ”¯æŒå…¬å¼", "æ˜“äºç¼–è¾‘"]
            },
            "html": {
                "description": "ç½‘é¡µæ˜¾ç¤ºæ ¼å¼",
                "use_case": "åœ¨çº¿é¢„è§ˆã€Webé›†æˆ",
                "features": ["å¯è§†åŒ–å±•ç¤º", "æ ·å¼ç¾åŒ–", "äº¤äº’æ”¯æŒ"]
            },
            "structured": {
                "description": "å®Œæ•´ç»“æ„åŒ–è¾“å‡º",
                "use_case": "å…¨æ ¼å¼éœ€æ±‚ã€ä¸€æ¬¡æ€§å¤„ç†", 
                "features": ["åŒ…å«æ‰€æœ‰æ ¼å¼", "å®Œæ•´ä¿¡æ¯", "çµæ´»ä½¿ç”¨"]
            }
        },
        "supported_elements": {
            "text": "æ–‡æœ¬å†…å®¹è¯†åˆ«",
            "table": "è¡¨æ ¼ç»“æ„è§£æ",
            "figure": "å›¾è¡¨/å›¾åƒåˆ†æ",
            "formula": "æ•°å­¦å…¬å¼è¯†åˆ«",
            "chart": "å›¾è¡¨æ•°æ®æå–",
            "diagram": "æµç¨‹å›¾ç†è§£"
        }
    }

# ç½‘é¡µç‰ˆç”¨æˆ·ç•Œé¢è·¯ç”±
@app.get("/web")
async def web_interface():
    """ç½‘é¡µç‰ˆç”¨æˆ·ç•Œé¢"""
    return FileResponse("src/web/index.html")


if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000) 